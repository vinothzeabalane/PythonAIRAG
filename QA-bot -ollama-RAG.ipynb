{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4084d1f-4be3-46d7-9135-1de66be740e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests numpy langchain langchain_huggingface langchain_community faiss openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d356d6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîπ Import necessary libraries\n",
    "import os\n",
    "import requests\n",
    "import numpy as np\n",
    "from typing import List\n",
    "from langchain_core.language_models import LLM\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.document_loaders import (\n",
    "    TextLoader,\n",
    "    PDFMinerLoader,\n",
    "    CSVLoader,\n",
    "    UnstructuredWordDocumentLoader,\n",
    "    UnstructuredPowerPointLoader,\n",
    ")\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "import faiss\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "086f199e-02ae-440f-84e0-8fe71b2d16e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîπ Custom LLM class to call Ollama locally\n",
    "class LocalOllamaLLM(LLM):\n",
    "    model_name: str = \"llama3.2:3b\"\n",
    "    url: str = \"http://localhost:11434/api/generate\"\n",
    "\n",
    "    def _call(self, prompt: str, stop: List[str] = None) -> str:\n",
    "        payload = {\n",
    "            \"model\": self.model_name,\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": False\n",
    "        }\n",
    "        headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "        try:\n",
    "            response = requests.post(self.url, json=payload, headers=headers, timeout=120)\n",
    "            response.raise_for_status()\n",
    "            result = response.json()\n",
    "            return result.get(\"response\", \"No response from AI.\")\n",
    "        except requests.RequestException as e:\n",
    "            return f\"Error: {str(e)}\"\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"ollama\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed18a7c1-cd22-4e16-8403-23a42b812f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîπ Load and split all supported documents\n",
    "def load_documents(directory=\"/home/remlab/ps-bpt/PythonAIRAG/data\"):\n",
    "    \"\"\"\n",
    "    Load and split supported documents (.txt, .pdf, .csv, .docx, .pptx) into chunks.\n",
    "    \"\"\"\n",
    "    all_docs = []\n",
    "    splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "\n",
    "    for file in os.listdir(directory):\n",
    "        path = os.path.join(directory, file)\n",
    "\n",
    "        try:\n",
    "            if file.endswith(\".txt\"):\n",
    "                loader = TextLoader(path)\n",
    "            elif file.endswith(\".pdf\"):\n",
    "                loader = PDFMinerLoader(path)\n",
    "            elif file.endswith(\".csv\"):\n",
    "                loader = CSVLoader(file_path=path)\n",
    "            elif file.endswith(\".docx\") or file.endswith(\".doc\"):\n",
    "                loader = UnstructuredWordDocumentLoader(path)\n",
    "            elif file.endswith(\".pptx\"):\n",
    "                loader = UnstructuredPowerPointLoader(path)\n",
    "            else:\n",
    "                print(f\"‚ùå Skipped unsupported file: {file}\")\n",
    "                continue\n",
    "\n",
    "            docs = loader.load()\n",
    "            chunks = splitter.split_documents(docs)\n",
    "            all_docs.extend(chunks)\n",
    "            print(f\"‚úÖ Loaded and split: {file}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Failed to process {file}: {e}\")\n",
    "\n",
    "    return all_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6156d202-05dd-415f-b0a0-01dd30fadb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîπ Main RAG pipeline\n",
    "def main():\n",
    "    # Step 1: Load and split all documents\n",
    "    documents = load_documents()\n",
    "\n",
    "    # Step 2: Initialize embedding model\n",
    "    embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "    # Step 3: Embed the document chunks\n",
    "    document_embeddings = [embedding_model.embed_query(doc.page_content) for doc in documents]\n",
    "    dim = len(document_embeddings[0])\n",
    "    index = faiss.IndexFlatL2(dim)\n",
    "\n",
    "    # Step 4: Convert to NumPy array and add to FAISS index\n",
    "    embeddings_np = np.array(document_embeddings).astype('float32')\n",
    "    index.add(embeddings_np)\n",
    "\n",
    "    # Step 5: Create in-memory docstore for mapping results\n",
    "    docstore = InMemoryDocstore({str(i): doc for i, doc in enumerate(documents)})\n",
    "    index_to_docstore_id = {i: str(i) for i in range(len(documents))}\n",
    "\n",
    "    # Step 6: Set up the FAISS vector store\n",
    "    vector_db = FAISS(\n",
    "        embedding_function=embedding_model,\n",
    "        index=index,\n",
    "        docstore=docstore,\n",
    "        index_to_docstore_id=index_to_docstore_id\n",
    "    )\n",
    "\n",
    "    # Step 7: Load the local LLM (Ollama)\n",
    "    llm = LocalOllamaLLM()\n",
    "\n",
    "    # Step 8: Build the RetrievalQA chain\n",
    "    rag_chain = RetrievalQA.from_chain_type(llm=llm, retriever=vector_db.as_retriever())\n",
    "\n",
    "    # Step 9: Ask a question (from documents)\n",
    "    question = \"Summarize the Chewy machine details.\"\n",
    "    result = rag_chain.invoke({\"query\": question})\n",
    "\n",
    "    # Step 10: Print the answer\n",
    "    print(\"\\nüìå Question:\", question)\n",
    "    print(\"ü§ñ Answer:\", result[\"result\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e51af39-a812-4bc2-b9e3-7605cf1f5e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded and split: DCA.pdf\n",
      "‚úÖ Loaded and split: lm-302-04-s2-2TB-EB0-2025-04-17.csv\n",
      "‚úÖ Loaded and split: ai.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 5594, which is longer than the specified 1000\n",
      "Created a chunk of size 5762, which is longer than the specified 1000\n",
      "Created a chunk of size 4254, which is longer than the specified 1000\n",
      "Created a chunk of size 1117, which is longer than the specified 1000\n",
      "Created a chunk of size 1349, which is longer than the specified 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded and split: SPI-winbond.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/remlab/ps-bpt/PythonAIRAG/.venv/lib/python3.8/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìå Question: Summarize the Chewy machine details.\n",
      "ü§ñ Answer: I can summarize the Chewy machine details from the provided context.\n",
      "\n",
      "The Chewy machine has a unique address of 10.74.133.95. It is not attached to anything and does not have any additional information listed next to it.\n",
      "\n",
      "However, there are other entries related to the Chewy machine, including:\n",
      "\n",
      "- vc-ps-chewy-13: connected to GHS V4 with IP address 10.74.135.35.\n",
      "- vc-ps-chewy-14: has a drive size of 16TB and is temporarily swapped with another drive on chewy-06.\n",
      "\n",
      "The exact details about the Chewy machine's capabilities, specifications, or connections are not explicitly stated in the provided context.\n"
     ]
    }
   ],
   "source": [
    "# üîπ Run the main function to start the process\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
